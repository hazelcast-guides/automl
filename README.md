# AutoML Proof of Concept

This project is a sandbox for learning how to operationalize a Google Cloud hosted, tabular ML classification model.  The topic problem is credit 
card fraud detection.  The model was trained on a dataset that was generated by https://github.com/namebrandon/Sparkov_Data_Generation with minor 
modifications, and the "real" transactions that are used to test the operationalized system come from the same source.

There are scripts and configurations to assist in running this sandbox with Docker compose or GKE.

The main components are as follows:

|File                      | Description                                                                                                                      |
|--------------------------|----------------------------------------------------------------------------------------------------------------------------------|
| compose.yaml             | Docker compose file describing how to run all components locally                                                                 |
| retrieve_gcp_creds.sh    | Logs in to Google Cloud and retrieves a credentials file, `application_default_credentials.json`. __Do not check this file in.__ |
| card-fraud.proto         | Protobuf definition of an authorization request                                                                                  |
| event-sender             | Contains the python program, `eventsender.py` for sending events to Hazelcast via map.put()                                      |
| config/hazelcast.yaml    | The configuration used by the Hazelcast instance                                                                                 | 
| scoring-pipeline         | Java code for the prediction pipeline                                                                                            |
| submitjob.sh             | Helper script to deploy the pipeline to Hazelcast via the Hazelcast CLI                                                          |

# Train a Model with AutoML and Expose it as an Endpoint

This step requires a Google Cloud account.  
1. In the console, go to the Vertex AI dashboard and select the "Tabular" type and upload the following data set from Google storage: 
`gs://rmay/automl/train.csv`
2. Select "Train New Model".  Select the "Classification" objective and the "AutoML" training method. 
3. On the next page, select `is_fraud` as the column containing the correct classification.
4. On the next page, you will select the fields to use for training purposes. All fields are selected by default.  Unselect the fields 
that are not listed in `card_fraud.proto`.  This is important because the generated test transactions will only contain those fields. 
 __See the note below__.  Continue without changing any advanced options.
5. On the next page, provide a training budget and start the training process. 

__NOTE ON SELECTING MODEL FIELDS__
This dialog is a little tricky.  The check boxes on the left aren't helpful for including and excluding fields.  The right most 
column consists of +/- buttons.  By default, everything is included so you need to click the +/- button on each row that should not
be a part of the model.  When a row is excluded, it is displayed in lighter gray font.  When you are done, check that each of the 
13 relevant fields is included in the model, as well as the "is_fraud" field for a total of 14.

Once the training is finished, you can expose the model as an endpoint using the following procedure.

1. From the Vertex AI dashboard, navigate to model repository > my model > select version > Deploy and Test > Deploy to Endpoint
2. Click through the wizard, selecting defaults.  Since this is not a production scenario, you can disable model monitoring.

Once the model has been deployed to an endpoint, note the region and the model-id.  These will be needed to configure the Hazelcast pipeline.

# Start Up a Local Hazelcast Instance and Send Auth Requests

`docker compose up -d`

This will bring up a local Hazelcast instance, an instance of management center, and a source of simulated auth requests.

Bring up the management center at http://localhost:8080.  Enable "Dev Mode", and add the local cluster by specifying "dev" for 
the cluster name and "hazelcast" for the member name.

You can use the management center UI to verify that entries are being placed in the `auth_requests` map at a rate of approximately 10/second.

# Submit the Jet Job to Process Transactions

Make sure the scoring pipeline jar is built.
```
cd scoring-pipeline
mvn package
cd ..
```

Now log in to GCP and obtain a credentials file.  You can use any account to access the endpoint but you must be authenticated. Use the 
following procedure to obtain a credentials file.
```
cd scoring-pipeline
./retrieve_gcp_creds.sh  # you will be asked to authenticate
cd ..
```

Now, edit the `submitjob.sh` file.  Enter the project name, region and model endpoint number of the trained model.  Tip: use the full
identifier of the project, not the short name.  You can see this by clicking on the project name in the top-left of the Google Cloud 
Console.


# Instructions

__This assumes a tabular classification ML model has already been trained and deployed to Google Cloud.__

## Build the java project.
```
cd scoring-pipeline
mvn clean package
```

## Generate the card transaction data.
This is the data that will be sent to Hazelcast for fraud detection.
Clone https://github.com/wrmay/Sparkov_Data_Generation, which is a fork of the original that uses commas instead of pipes for separating data. This was a 
requirement of Google's AutoML.

Generate the data with something similar to the following: 
```
python -m venv venv
. venv/bin/activate
pip install -r requirements.txt
python datagen.py -n 10000 -o data 01-01-2022 01-31-2022
```
Note: attempts to generate less than 8 days of data will fail, pick the start and end date accordingly

Use `event-sender/csv_sort.py` to restore all of the *nnnnn.csv files by the linux timestamp column and remove the header row (_needs documentation_).

Copy all of the generated data files into the `data/transactions_for_generator` project.

## Start the Simulation
```
docker compose up -d
```

The management center should be available at `localhost:8080`

## Preparation
Obtain Google Cloud credentials.  By default, any authenticated user can access the model.
```
./retrieve_gcp_creds.sh
```
This should create a file, `application_default_credentials.json`, which you __should not check in to github__.

Obtain the project, location and endpoint id of the tabular, classification model endpoint you will access.
For example: "hazelcast-33", "us-central1", "4731246912831750144".  Edit `submitjob.sh` accordingly.

## Submit the Pipeline
```
./submitjob.sh 
```


