= Using AutoML in a Hazelcast Pipeline
:page-layout: tutorial
:page-product: platform
:page-categories: Machine Learning, Google Cloud Platform
:page-lang: java 
:page-enterprise: 
:page-est-time: 60 minutes
:description: Train a model using Google's AutoML and incorporate it into a Hazelcast model serving pipeline.

{description}

== Context

This example demonstrates using a Hazelcast Pipeline as an ML scoring service.  You will learn the basics of Hazelcast Pipelines and how to integrate with an external 
service.  

NOTE: A typical ML pipeline would include data manipulation and augmentation tasks to develop a feature vector from incoming events, and a scoring task that involves using an ML model to make a prediction.  In this tutorial, we are focused only on the scoring tasks and we will leverage a model trained and hosted in Google Cloud.

The diagram below is a schematic of what we will build in this tutorial.

image::pipeline.png[Pipeline]

== Before you Begin

You will need the following prerequisites to complete this tutorial.

* A Google Cloud Platform account (this will be used to develop the model and host it on an endpoint)
* Docker Desktop on your local machine
* Maven on your local machine
* A Java IDE 
* Basic knowledge of Java.  Knowledge of functional programming constructs will be 
especially helpful.

== Step 1. Train a Model with VertexAI 

In this step, we will create a simple ML model for fraud detection and train it using a publicly available data set. 

. Sign in to the Google Cloud Console (console.cloud.google.com)  and navigate to the Vertex AI dashboard
. Click "Create Dataset" and select the "Tabular" type.  Upload the following data set from Google storage: `gs://rmay/automl/train.csv`
. Once the upload completes, select "Train New Model".  Select the "Classification" objective and the "AutoML" training method. 
. On the next page, select `is_fraud` as the column containing the correct classification.
. On the next page, select the fields to use for training purposes. Note that all 
fields are selected by default.  *Unselect* the fields that are not listed in `card_fraud.proto` (copied below). When only the listed fields are selected,
click "Continue".
+
```
message AuthRequestv01 {
    string gender = 1;
    string city = 2;
    string state = 3;
    string lat = 4;
    string long = 5;
    string city_pop = 6;
    string job = 7;
    string dob = 8;
    string category = 9;
    string amt = 10;
    string merchant = 11;
    string merch_lat = 12;
    string merch_long = 13;
}
```
NOTE: This step is mandatory because the training data you use in this step must match the data that is provided during the scoring step.  However, this dialog is a little tricky.  The check boxes on the left aren't helpful for including and excluding fields.  The right most column consists of +/- buttons.  By default, everything is included so you need to click the +/- button on each row that should not be a part of the model.  When a row is excluded, it is displayed in lighter gray font.  When you are done, check that each of the 13 relevant fields is included in the model, as well as the "is_fraud" field for a total of 14.
. On the next page, provide a training budget and start the training process. A training budget of 1 hr. is sufficient to obtain a usable model.  

+
TIP: This is a good time to go get coffee or tea!  Given a one hour training budget, the process will take 90-120 minutes.


== Step 2. Deploy the Model to an Endpoint

Once the training is finished, you can expose the model as an endpoint using the following procedure.

. From the Vertex AI dashboard, navigate to model repository > my model > select version > Deploy and Test > Deploy to Endpoint
. Click through the wizard, selecting defaults.  Since this is not a production scenario, disable model monitoring.

Once the model has been deployed, note the region and the model-id.  These will be needed to configure the Hazelcast pipeline.

== Step 3. Start Hazelcast, the Data Loader and Management Center 

In this step, you will deploy a single-node cluster and Hazelcast Management Center using the standard Docker images. Run the following commands to start everything:

```bash
docker compose up -d
```

NOTE: If you wish to see how each process is started, see `compose.yaml`

Within a few minutes, you should be able to access the Hazelcast Management Center 
at http://localhost:8080. You will need to click on "Enable Dev Mode", which starts 
Management Center with no authentication.  Use management center to verify that 
there is map named `auth_requests` which is receiving traffic.  

You can also use management center to verify that there are currently no jobs
running.  The relevant sections are highlighted on the image below.

image::mancenter.png[Management Center]

== Step 4.Build and Deploy a Scoring Pipeline

In this step, you will build the scoring pipeline, package it as a jar and deploy it to the running  cluster using the Hazelcast CLI.

During this exercise, the following references will be essential.

* https://docs.hazelcast.com/hazelcast/latest/pipelines/overview
* https://docs.hazelcast.org/docs/latest/javadoc//index.html?com/hazelcast/jet/pipeline/Sources.html
* https://docs.hazelcast.org/docs/latest/javadoc//index.html?com/hazelcast/jet/pipeline/Sinks.html
* https://docs.hazelcast.org/docs/latest/javadoc/index.html?com/hazelcast/jet/pipeline/StreamStage.html
* https://docs.hazelcast.org/docs/latest/javadoc//index.html?com/hazelcast/jet/datamodel/Tuple2.html


First, let's take a quick tour of the code base.  The skeleton for the scoring 
pipeline is in  `scoring-pipeline`, which is just a maven based Java project. 

[horizontal]
com.hzsamples.automl.PredictionPipeline.java:: This is where you will implement the scoring pipleine.  The `buildPipeline` method constructs the pipeline.
com.hzsamples.automl.AutoMLTabularPredictionClient:: This helper class wraps the Google API and simplifies 
certain tasks such as authentication.
com.hzsamples.automl.solution.PredictionPipeline.java:: A working solution for 
your reference.

In the root directory, there are 2 scripts which you can use to submit and cancel 
your pipeline.

[horizontal]
submitjob.sh:: Submits the pipeline to the running Hazelcast cluster.  Note that this 
script passes in pointers to the Google Cloud endpoint which you will need to edit.
canceljob.sh:: Cancels the running job.

In this tutorial, we will take an interative approach. Generally, the process will 
follow these steps: code > build > deploy > test > undeploy > repeat .

=== Setup
. Before getting started, edit `submitjob.sh` to include the correct project, region 
and endpoint id for your model.  

TIP: Use the full name of your GCP project, not the short name that displays at t
the top-left of your cloud console window.  You can hover over the project selection 
drop-down to see the full name of your project.

[start=2]
. You will also need to sign in to your Google Cloud account and obtain credentials.
The credentials will be used by the Hazelcast Pipeline to access the model endpoint.

[source, bash]
----
cd scoring-pipeline
./retrieve_gcp_credentials.sh
cd ..
----

Verfiy that you now have a `gcp-credentials.json` file in the `scoring-pipeline` 
directory.

[start=3]
. Compile the `scoring-pipeline` project.  This will ensure that generate the 
protobuf-defined classes so they will be available before you open the 
poject in an IDE.
[source, bash]
----
cd scoring-pipeline
mvn compile
cd ..
----


=== Create a Stream Source to Read the Data

Now open the `scoring-pipeline` project in an IDE.

In the `buildPipeline` method, use `readFrom` to read events from the `auth_requests`
map.  The key of this map is a String and the value is a protobuf-serialized 
`AuthRequest` message as defined in `card-fraud.proto`.


[source, java]
----
StreamStage<Map.Entry<String, byte[]>> serializedAuthRequests = result.readFrom(
        Sources.<String, byte[]>mapJournal("auth_requests", JournalInitialPosition.START_FROM_OLDEST))
        .withIngestionTimestamps().setName("Input");
----

Next, use the `StreamStage.map` method to unpack the byte array into a POJO:
 `AuthRequestV0`.

[source, java]
----
StreamStage<AuthRequestv01> authRequests =
        serializedAuthRequests.map(entry -> AuthRequestv01.parseFrom(entry.getValue()))
                .setName("deserialize Proto");
----

Now, write each event to a log so we can see what we have so far.

[source, java]
----
authRequests.writeTo(Sinks.logger());
----

Now let's build and deploy the pipeline.  

[source, bash]
----
cd scoring-pipeline
mvn package
cd ..
./submitjob.sh
# wait for the job to be deployed, then check the logs
docker compose logs --follow hazelcast
----

You should see output similar to what is shown below.
[source]
----
automl-hazelcast-1  | 2022-12-20 19:30:17,747 [ INFO] [hz.stoic_diffie.jet.blocking.thread-6] [c.h.j.i.c.WriteLoggerP]: [172.22.0.5]:5701 [dev] [5.2.0] [fraud-prediction/loggerSink#0] gender: "F"
automl-hazelcast-1  | city: "Royal Oak"
automl-hazelcast-1  | state: "MI"
automl-hazelcast-1  | lat: "42.4906"
automl-hazelcast-1  | long: "-83.1366"
automl-hazelcast-1  | city_pop: "57256"
automl-hazelcast-1  | job: "Insurance claims handler"
automl-hazelcast-1  | dob: "1950-12-23"
automl-hazelcast-1  | category: "grocery_pos"
automl-hazelcast-1  | amt: "64.33"
automl-hazelcast-1  | merchant: "fraud_Stracke-Lemke"
automl-hazelcast-1  | merch_lat: "43.085576"
automl-hazelcast-1  | merch_long: "-82.627076\n"
----

Finally, use the `canceljob.sh` script to undeploy the job before continuing.  

=== Format the Request and Call the Model Endpoint on GCP

In the previous step, we learned that `readFrom` is used to begin a pipeline and 
`writeTo` terminates a pipeline, sending processed events or decisions to an 
external system (or just logging them in this case).  We also saw that a `map` step 
is used to transform the events in the stream. In this step, we will use 
`mapUsingService` to transform the events based on an external service.  

Generally, when we connect to any sort of external service, there is some cost 
to creating a connection and we don't want to do that each time we process an 
event.  This is the reason we need `mapUsingService`. We first create a service 
by telling Hazelcast how to connect.

[source, java]
----
ServiceFactory<?, AutoMLTabularPredictionClient> predictionService =
        ServiceFactories.nonSharedService(c -> new AutoMLTabularPredictionClient(
                modelProject,
                modelLocation,
                modelEndpointId,
                credentials)).toNonCooperative();
----

One service instance will be created in each node (this can be cofigured).  Once 
created, service instances are reused.  

The next 2 steps are converting the `AuthRequestV0` pojo into the generic 
protobuf map required by the endpoint, and then calling the service.  This 
is done as shown below.  Lastly, to aid with debugging, you can change the 
final `writeTo` step to write the results of the prediction.

[source, java]
----
// extract the fields of interest from the POJO and format as a protobuf Struct as required for the Vertex AI endpoint
StreamStage<Tuple2<AuthRequestv01, Struct>> authReqProtos =
        authRequests.map(authReq -> tuple2(authReq, authRequestToFeature(authReq)))
                .setName("map to predict api features");

// use the custom prediction service to obtain a PredictionResponse, wrap the response in a helper class for ease of use
StreamStage<Tuple2<AuthRequestv01, AutoMLTabularPredictionClient.PredictResponseExtractor>> predictions
        = authReqProtos.mapUsingService(predictionService, (ps, tuple) -> tuple2(tuple.f0(), ps.predict(tuple.f1())))
        .setName("call predict api");

predictions.writeTo(Sinks.logger());
----

You can now build and deploy as you did before.  You should see output similar to
the following.

[source]
----
automl-hazelcast-1        | 2022-12-20 20:24:23,353 [ INFO] [hz.stoic_diffie.jet.blocking.thread-10] [c.h.j.i.c.WriteLoggerP]: [172.22.0.5]:5701 [dev] [5.2.0] [fraud-prediction/loggerSink#0] (gender: "M"
automl-hazelcast-1        | city: "Mason"
automl-hazelcast-1        | state: "OH"
automl-hazelcast-1        | lat: "39.3357"
automl-hazelcast-1        | long: "-84.3149"
automl-hazelcast-1        | city_pop: "50627"
automl-hazelcast-1        | job: "Chartered accountant"
automl-hazelcast-1        | dob: "2001-02-11"
automl-hazelcast-1        | category: "kids_pets"
automl-hazelcast-1        | amt: "141.29"
automl-hazelcast-1        | merchant: "fraud_Hilpert-Conroy"
automl-hazelcast-1        | merch_lat: "38.756688"
automl-hazelcast-1        | merch_long: "-85.314782\n"
automl-hazelcast-1        | , PredictResponseExtractor{response=predictions {
automl-hazelcast-1        |   struct_value {
automl-hazelcast-1        |     fields {
automl-hazelcast-1        |       key: "classes"
automl-hazelcast-1        |       value {
automl-hazelcast-1        |         list_value {
automl-hazelcast-1        |           values {
automl-hazelcast-1        |             string_value: "0"
automl-hazelcast-1        |           }
automl-hazelcast-1        |           values {
automl-hazelcast-1        |             string_value: "1"
automl-hazelcast-1        |           }
automl-hazelcast-1        |         }
automl-hazelcast-1        |       }
automl-hazelcast-1        |     }
automl-hazelcast-1        |     fields {
automl-hazelcast-1        |       key: "scores"
automl-hazelcast-1        |       value {
automl-hazelcast-1        |         list_value {
automl-hazelcast-1        |           values {
automl-hazelcast-1        |             number_value: 0.9885889291763306
automl-hazelcast-1        |           }
automl-hazelcast-1        |           values {
automl-hazelcast-1        |             number_value: 0.01141107082366943
automl-hazelcast-1        |           }
automl-hazelcast-1        |         }
automl-hazelcast-1        |       }
automl-hazelcast-1        |     }
automl-hazelcast-1        |   }
automl-hazelcast-1        | }
automl-hazelcast-1        | deployed_model_id: "1479938252939984896"
automl-hazelcast-1        | model: "projects/1097314672797/locations/us-central1/models/54537975760945152"
automl-hazelcast-1        | model_display_name: "fraud_detection_2"
automl-hazelcast-1        | model_version_id: "1"
automl-hazelcast-1        | , succeeded=true})
----

=== Finish the Pipeline

As you can see from the output, calling the prediction API does not yield a decision 
but a set of probabilities associated with each class.   In this case the classes are 
"1" for fraudulent and "0" for non-fraudulent. In this step we will finish the 
Pipeline by turning result from the model endpoint into a decision.

First, we use a new verb, `filter` to eliminate those prediction results that 
failed for any reason.  Note that, with a `map` operation, the number of events
in and out are equal.  With the `filter` operation, the number of events 
exiting the stage can be less than those entering the stage.

[source, java]
----
StreamStage<Tuple2<AuthRequestv01, AutoMLTabularPredictionClient.PredictResponseExtractor>> goodPredictions
        = predictions.filter(t2 -> t2.f1().isSucceeded()).setName("filter out exceptions");
----

To finish up, we need to turn the list of probabilities into a decision and 
log the result. 

[source, java]
----
// based on the confidence scores in the prediction response, make a fraud prediction
StreamStage<Tuple2<AuthRequestv01, Boolean>> decisions =
        goodPredictions.map((tuple) -> tuple2(tuple.f0(), tuple.f1().getPrediction(0, "1") > fraudConfidenceThreshold))
                .setName("classify");

// output the decision
decisions.writeTo(Sinks.logger((t) -> (t.f1() ? "DECLINED " : "APPROVED") + t.f0().getAmt() + " on " + t.f0().getCategory() + " in " + t.f0().getCity() + ", " + t.f0().getState()));
----

Re-deploy the job once more to see the final result.

[source]
----
automl-hazelcast-1        | 2022-12-20 20:44:37,280 [ INFO] [hz.stoic_diffie.jet.blocking.thread-16] [c.h.j.i.c.WriteLoggerP]: [172.22.0.5]:5701 [dev] [5.2.0] [fraud-prediction/loggerSink#0] APPROVED60.52 on grocery_net in Grayling, MI
----

You can also log in to the Google Cloud Console and check on the end point.

image::gcp.png[GCP]

Congratulations!  You've finished this tutorial.

== Summary

This tutorial was a basic introduction to using Hazelcast Pipelines for 
operationalizing ML and specifically for incorporating Vertex API endpoints 
hosted on GCP. 

== Credits

The data for this data set was generated by https://github.com/wrmay/Sparkov_Data_Generation, which is a fork of 
https://github.com/namebrandon/Sparkov_Data_Generation with very minor modifications.

